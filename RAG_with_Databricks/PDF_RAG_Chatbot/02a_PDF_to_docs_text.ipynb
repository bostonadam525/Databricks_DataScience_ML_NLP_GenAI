{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966e3ffd-f9e7-4135-a218-1064d765a08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2a - PDF to docs_text\n",
    "* Notebook by Adam Lang\n",
    "* Notebook was adopted from the Databricks webinar in June 2024 that streamed on the Databricks YouTube channel.\n",
    "* This is the 2nd notebook and the first step after creating the tables within the unity catalog. \n",
    "* Date: 4/28/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23e7932-9bef-4f94-9176-0dbe232af824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "### Note about PDF Extraction Library - `PdfPlumbr`\n",
    "* `PdfPlumbr` is a library that allows you to extract text, tables, and metadata from PDFs. \n",
    "* It provides a simple API to work with and is particularly good at handling complex PDF layouts.\n",
    "* Overall it provides a simple and intuitive interface for extracting text, images, and layout information from PDF documents. \n",
    "* It also supports advanced features like table detection and text extraction from complex layouts.\n",
    "* There are numerous other extraction libraries available such as the newer `PyMuPDF4LLM` which uses an LLM under the hood. \n",
    "* Here is a full list of options in python for [PDF extraction options](https://medium.com/@bojjasharanya/automating-pdf-data-extraction-your-ultimate-guide-for-choosing-the-suitable-library-d87a3dcf27e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6eaa46c-fe93-4fb8-946f-0519dffc0e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pdfplumber in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (0.11.6)\nRequirement already satisfied: langchain in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (0.3.24)\nRequirement already satisfied: pdfminer.six==20250327 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from Pdfplumber) (20250327)\nRequirement already satisfied: Pillow>=9.1 in /databricks/python3/lib/python3.11/site-packages (from Pdfplumber) (10.3.0)\nRequirement already satisfied: pypdfium2>=4.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from Pdfplumber) (4.30.1)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from pdfminer.six==20250327->Pdfplumber) (2.0.4)\nRequirement already satisfied: cryptography>=36.0.0 in /databricks/python3/lib/python3.11/site-packages (from pdfminer.six==20250327->Pdfplumber) (41.0.3)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain) (0.3.56)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain) (0.3.8)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain) (0.3.38)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain) (2.11.3)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /databricks/python3/lib/python3.11/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: PyYAML>=5.3 in /databricks/python3/lib/python3.11/site-packages (from langchain) (6.0)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (8.2.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /databricks/python3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (23.2)\nRequirement already satisfied: typing-extensions>=4.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /databricks/python3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\nRequirement already satisfied: greenlet>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->Pdfplumber) (1.15.1)\nRequirement already satisfied: anyio in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->Pdfplumber) (2.21)\nRequirement already satisfied: sniffio>=1.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6ff9b926-1fbb-41da-a83f-46586f22ec40/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install Pdfplumber langchain\n",
    "\n",
    "## then restart kernel\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aad0a12-e7cc-4d5b-b69d-7e8c6027f0c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create Dataframe for file storage\n",
    "* This will show us what files we currently have in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d658fe5c-f5a8-4fee-8b8b-f7038ea2aac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washer_Manual.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           file_name\n",
       "0  Washer_Manual.pdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from pyspark.sql.functions import substring_index \n",
    "\n",
    "# Set directory path\n",
    "dir_path = \"/Volumes/workspace/llm_rag_demos/pdf_rag_dbrx\"\n",
    "\n",
    "# list the files in the directory\n",
    "file_paths = [file.path for file in dbutils.fs.ls(dir_path)]\n",
    "\n",
    "# Extract file names from paths --> spark df\n",
    "df = spark.createDataFrame(file_paths, \"string\").select(substring_index(\"value\", \"/\", -1).alias(\"file_name\"))\n",
    "\n",
    "# Show df\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3bd132e-5265-4785-8ea4-fb15258302d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summary\n",
    "* We can see there is only 1 PDF document currently in the file store. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f412959e-d789-4bba-820e-9dcfb53a1c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Process new PDF Files -- Split & Chunk Text\n",
    "* Important Distinctions about RecursiveCharacterTextSplitter and TokenTextSplitter\n",
    "* The [ChromaDB research paper](https://research.trychroma.com/evaluating-chunking#langchain_recursive_text_splitter) took a critical look at the 2 most popular methods for chunking:\n",
    "\n",
    "1. RecursiveCharacterTextSplitter\n",
    "2. TokenTextSplitter\n",
    "\n",
    "These 2 methods are some of the most popular chunking methods, and the default used by many RAG systems.\n",
    "\n",
    "However, these chunking methods are insensitive to the **semantic content of the corpus**, relying instead on the position of character sequences to divide documents into chunks, up to a maximum specified length.\n",
    "\n",
    "ChromaDB utilized the very popular implementation of the Langchain library.\n",
    "\n",
    "Their findings from the paper above were very interesting:\n",
    "  * They found that it was necessary to alter some defaults to achieve fair results\n",
    "  * By default, the `RecursiveCharacterTextSplitter` uses the following separators: [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "  * They found this would commonly result in very short chunks, which performed poorly in comparison to TokenTextSplitter which produces chunks of a fixed length by default.\n",
    "  * Therefore, they used these additional sentence based separators: [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"] to improve chunk quality and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a4e3e5-3656-4150-8b83-52ce2e676320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "\n",
    "## set path of PDF volume store\n",
    "pdf_volume_path = \"/Volumes/workspace/llm_rag_demos/pdf_rag_dbrx\"\n",
    "\n",
    "## List PDF files already processed in the Delta Table\n",
    "processed_files = spark.sql(f\"SELECT DISTINCT file_name FROM workspace.llm_rag_demos.docs_track\").collect()\n",
    "processed_files = set(row[\"file_name\"] for row in processed_files)\n",
    "\n",
    "## Process ONLY the NEW PDF files\n",
    "new_files = [file for file in os.listdir(pdf_volume_path) if file not in processed_files]\n",
    "\n",
    "## init empty string to store all text extracted from NEW PDF files\n",
    "all_text = ''\n",
    "## loop through and extract text from each PDF file\n",
    "for file_name in new_files:\n",
    "  # Extract text from PDF file\n",
    "  pdf_path = os.path.join(pdf_volume_path, file_name)\n",
    "\n",
    "  with pdfplumber.open(pdf_path) as pdf:\n",
    "    for pdf_page in pdf.pages:\n",
    "      single_page_text = pdf_page.extract_text()\n",
    "      ## separate each page's text with newline\n",
    "      all_text = all_text + '\\n' + single_page_text\n",
    "\n",
    "# Split combined text into chunks using RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "## init text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, # character length\n",
    "                                               chunk_overlap=200,  # Overlap\n",
    "                                               length_function=len,  # Character length with len()\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"] # adding ChromaDB separators\n",
    ")\n",
    "\n",
    "## create chunks\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66978e5c-456e-43a1-a1d5-8b4a696dafb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Pandas UDF to chunk text data for insertion\n",
    "* We can then use a Pandas UDF when inserting our text into the docs_track table. \n",
    "* A pandas user-defined function (UDF)—also known as vectorized UDF—is a user-defined function that uses Apache Arrow to transfer data and pandas to work with the data. \n",
    "* pandas UDFs allows vectorized operations that can increase performance up to 100x compared to row-at-a-time Python UDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ced3f9-b0ba-425f-91da-9453219cec18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.split_chunks_udf(dummy)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "# create pandas udf function\n",
    "@pandas_udf('array<string>')\n",
    "def split_chunks_udf(dummy):\n",
    "    return pd.Series([chunks])\n",
    "  \n",
    "# Register the UDF \n",
    "spark.udf.register('split_chunks_udf', split_chunks_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de9b79ce-6e1f-47db-8f27-8012f1f5da3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>295</td><td>295</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         295,
         295
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 6
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "insert into workspace.llm_rag_demos.docs_text (text)\n",
    "select explode(split_chunks_udf('dummy')) as text;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79728187-04c1-4f04-9f36-b2c97e11aea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summary\n",
    "* We have now created 295 rows of text chunks from the original PDF document as we can see above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22daf151-7c89-47f5-aaaa-881ccd477e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Finally, we create a temporary table from the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f023cdd-0e13-4f03-957a-155afb8c0805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create temporary table from df\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "# Insert only rows that do not already exist in target table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO workspace.llm_rag_demos.docs_track\n",
    "    SELECT * FROM temp_table\n",
    "    WHERE NOT EXISTS (\n",
    "      SELECT 1 FROM workspace.llm_rag_demos.docs_track\n",
    "      WHERE temp_table.file_name = workspace.llm_rag_demos.docs_track.file_name\n",
    "    )\n",
    "          \n",
    "          \n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f079ad-48f6-4ffe-b880-306cfc902ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Summary\n",
    "* Updated the docs_track table with the chunks we created. \n",
    "* Notice we used IF NOT EXISTS so that if the chunks do already exist we won't insert them again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea40fb4d-242d-4c00-8287-a0ed2a578675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "fde6a80c-bff2-4d79-9683-938d4c48813b",
     "origId": 5282819306779643,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5282819306779630,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02a_PDF_to_docs_text",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}