{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7f2f532-462a-4737-a8d6-b6003099dfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2b - Web Scraping CSV to docs_text delta table\n",
    "* Notebook by Adam Lang\n",
    "* Notebook was adopted from the Databricks webinar in June 2024 that streamed on the Databricks YouTube channel.\n",
    "  * This is the alternative 2nd notebook and the first step after creating the tables within the unity catalog, but ONLY USE THIS NOTEBOOK if you are uploading a CSV file that you precreated such as from web scraping or other data source, not from a PDF file as the previous notebook. \n",
    "\n",
    "* Date: 4/30/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ab8d2a0-cd22-4793-9924-f9ada2c4b061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Install Dependencies\n",
    "* Install then restart kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d4d62f9-1a9d-47bd-8235-92dcc14527a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc73c603-dc3e-41d7-89d1-644365cb1b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Extract, Split and Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb596ec-9ca3-4120-ac26-6de2404baf1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain.text_splitter import RecursiveCharacterTextsplitter\n",
    "\n",
    "## 1. Read in csv text file with scraped web text\n",
    "df = spark.read.text(\"<insert csv file path here.csv>\")\n",
    "\n",
    "## 2. Collect all text into single string\n",
    "text_col = \" \".join([row.value for row in df.collect()])\n",
    "\n",
    "## 3. Setup Text splitter\n",
    "## init text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, # character length\n",
    "                                               chunk_overlap=200,  # Overlap\n",
    "                                               length_function=len,  # Character length with len()\n",
    "                                               separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"] # adding ChromaDB separators\n",
    ")\n",
    "\n",
    "## create chunks from text column\n",
    "chunks = text_splitter.split_text(text_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c2a7a2-f3c9-41fe-a747-04c0dff1facb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Create Pandas UDF to chunk text for insert to delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3731e4ad-0964-4c75-ba73-a47d152475f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf \n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import pandas as pd \n",
    "\n",
    "# setup chunk function\n",
    "@pandas_udf(\"array<string>\")\n",
    "def get_chunks(dummy) -> pd.Series:\n",
    "    return pd.Series([chunks])\n",
    "\n",
    "# Register the UDF dataframe\n",
    "spark.udf.register(\"get_chunks\", get_chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14aa3ac9-4242-4c96-9d19-a15dd2ea9218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Load/Insert chunked data into docs_text delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e42fec19-0911-4ef7-adac-987923196406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "insert into workspace.llm_rag_demos.docs_text (text)\n",
    "select explode(get_chunks('dummy')) as text;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2b - Web Scraping CSV to docs_text delta table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}